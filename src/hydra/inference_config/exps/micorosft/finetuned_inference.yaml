# @package _global_

defaults:
  - override /data:  ids_data
  - override /model: gpt

project: 'FinetuneMicrosoft'
experiment: 'finetune_microsoft_seed1' 

data: 
  artifact_name: Microsoft-test-20250721_160309
  ground_truth_length: 110 
  block_size: 1500
  batch_size: 30
  test_seed: 34721
  observation_size: 10
  lower_bound_obs_size: 2
  test_dataset_size: 5000


model: 
  checkpoint_path: <your.storage.path>/model_checkpoints/FinetuneMicrosoft/microsoft_data_nuc_CPRED_observation_size_10_ground_truth_110/finetune_microsoft_seed1/train_run_gpt_20250918_210827/checkpoint_best.pt
  sampling:
    temperature: 1.0
    top_k: None
    strategy: greedy      # or “beam_search”
    max_new_tokens: ${data.ground_truth_length}
    constrained_generation: False
    # Attention tracking parameters
    track_attention: false
    detailed_attention: false
    attention_output_dir: "<your.storage.path>/attention_results/finetune_mic_eval_mic" # Relative - creates under current working directory
    # Entropy tracking parameters
    track_entropy: false
    entropy_analysis: false
    log_entropy_details: false  # Set to true for per-example entropy logging
    # Sequence output parameters
    save_sequences: true  # Set to true to save GT and predictions for failure analysis
    sequence_output_file: "<your.storage.path>/pred_gt/finetune_microsoft/predictions_output.tsv"  # File to save sequences
