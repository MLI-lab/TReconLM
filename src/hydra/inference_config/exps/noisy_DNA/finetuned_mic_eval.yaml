# @package _global_

defaults:
  - override /data:  ids_data
  - override /model: gpt

project: 'CrossDomain'
experiment: 'finetune_noisy_eval_mic' 

data: 
  artifact_name: Microsoft-test-20250721_160309 
  ground_truth_length: 110 
  block_size: 1500
  batch_size: 30
  test_seed: 34721
  observation_size: 10
  lower_bound_obs_size: 2
  test_dataset_size: 50000
  cross: microsoft  # Options: null, 'noisy', 'microsoft'
  subsample_large_clusters: false # If true, randomly subsample clusters 9-10 to 2-5 reads


model: 
  checkpoint_path: <your.storage.path>/model_checkpoints/FinetuneNoisyDNA/noisy_data_nuc_CPRED_observation_size_10_ground_truth_60/finetune_noisy_padded110/train_run_gpt_20250917_112617/checkpoint_best.pt
  sampling:
    temperature: 1.0
    top_k: None
    strategy: greedy      # or “beam_search”
    max_new_tokens: ${data.ground_truth_length}
    constrained_generation: False
    # Attention tracking parameters
    track_attention: true
    detailed_attention: true
    attention_output_dir: "<your.storage.path>/attention_results/finetune_noisy_eval_mic" # Relative - creates under current working directory
    # Entropy tracking parameters
    track_entropy: true
    entropy_analysis: true
    log_entropy_details: false  # Set to true for per-example entropy logging
    exchange_positional_encoding: <your.storage.path>/model_checkpoints/FinetuneMicrosoft/microsoft_data_nuc_CPRED_observation_size_10_ground_truth_110/finetune_microsoft_seed1/train_run_gpt_20250918_210827/checkpoint_best.pt
