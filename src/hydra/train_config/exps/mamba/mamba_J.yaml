# @package _global_

defaults:
  - override /general: scratch          
  - override /data:    ids_data
  - override /model:   mamba          
  - override /train:   base

project:   TRACE_RECONSTRUCTION
experiment: mamba_J_1e17       

general:
  checkpoint_path: <your.storage.path>                # fill in if you have a custom scratch path
  #train_time: ""                      # only needed when resuming

data:
  sequence_type:       nuc
  target_type:         CPRED
  observation_size:    10
  ground_truth_length: 110
  lower_bound_obs_size: 2
  block_size:          1500
  test:                true
  test_seed:           34721
  test_dataset_size:   50000

train:
  ddp:                    true
  dtype:                  bfloat16   # or float16 / float32
  eval_interval:          500
  log_interval:           10
  eval_iters:             1000
  eval_only:              false
  always_interval:        500
  always_save_checkpoint: true
  device:                 cuda:0
  gradient_accumulation_steps: 1
  batch_size:             16
  learning_rate:          0.0001
  max_iters:              98114          # same “1 e17‑token” target
  weight_decay:           0.1
  beta1:                  0.9
  beta2:                  0.95
  grad_clip:              1.0
  decay_lr:               true
  warmup_iters:           0              # set in code to 5 % of max_iters
  min_lr:                 0.0
  lr_decay_iters:         ${train.max_iters}

model:
  mamba_params:
    d_state:          32
    n_layer:          4
    d_model:          384
    d_intermediate:   1536
    rms_norm:         true
    residual_in_fp32: false

    ssm_cfg:
      d_state:        32
      d_conv:         4
      expand:         2
      dt_min:         0.001
      dt_max:         0.1
      dt_init_floor:  1e-4
      conv_bias:      true
      bias:           false
      layer:          Mamba2 default to Mamba 1

    attn_layer_idx: []          # if you want any transformer‐style attention layers
    attn_cfg:       {}
    fused_add_norm: true
    pad_vocab_size_multiple: 8
    tie_embeddings: true


wandb:
  wandb_log: true
